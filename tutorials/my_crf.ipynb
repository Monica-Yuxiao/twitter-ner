{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x112be91b0>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch.optim import Adam\n",
    "import numpy as np\n",
    "\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1.79175947 -3.21887582]\n",
      " [-1.79175947 -3.21887582]\n",
      " [-1.79175947 -3.21887582]\n",
      " [-1.79175947 -3.21887582]\n",
      " [-1.79175947 -3.21887582]\n",
      " [-1.79175947 -0.22314355]]\n"
     ]
    }
   ],
   "source": [
    "# two dice one is fair, one is loaded\n",
    "# emission matrix i.e. p(observing a 0|fair dice)\n",
    "fair_dice = np.array([1/6]*6)\n",
    "loaded_dice = np.array([0.04,0.04,0.04,0.04,0.04,0.8])\n",
    "\n",
    "probabilities = {'fair': fair_dice,\n",
    "                'loaded': loaded_dice}\n",
    "# print(fair_dice.reshape(-1,1))\n",
    "# print(loaded_dice.reshape(-1,1))\n",
    "# h1 = fair_dice.reshape(-1,1)\n",
    "# h2 = loaded_dice.reshape(-1,1)\n",
    "# print(np.hstack([h1, h2]))\n",
    "log_likelihood = np.hstack([np.log(fair_dice).reshape(-1,1),\n",
    "                            np.log(loaded_dice).reshape(-1,1)])\n",
    "\n",
    "# 1st col: log(observing 1/2/3/4/5/6 of a fair dice); 2nd col: log(observing 1/2/3/4/5/6 of a loaded dice)\n",
    "print(log_likelihood)\n",
    "\n",
    "\n",
    "# transition matrix between states (start, fair dice, loaded dice)\n",
    "# if dice is fair at time t, 0.6 chance we stay fair, 0.4 chance it is loaded at time 2\n",
    "transition_mat = {'fair': np.array([0.6, 0.4, 0.0]),\n",
    "                 'loaded': np.array([0.3, 0.7, 0.0]),\n",
    "                 'start': np.array([0.5, 0.5, 0.0])}\n",
    "states = ['fair', 'loaded', 'start']\n",
    "state2ix = {'fair': 0,\n",
    "           'loaded': 1,\n",
    "           'start': 2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "-1.1523 -0.9618 -1.1028\n",
      "-1.0563 -1.0892 -1.0058\n",
      "[torch.FloatTensor of size 2x3]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# initialize transition matrix\n",
    "# p(i, j) = p(current state is i | prev state is j)\n",
    "n_dice = 2\n",
    "n_states = n_dice\n",
    "transition = torch.nn.init.normal(nn.Parameter(torch.randn(n_dice, n_dice + 1)), -1, 0.1)\n",
    "print(transition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CRF(nn.Module):\n",
    "    \n",
    "    def __init__(self, n_dice, log_likelihood):\n",
    "        super(CRF, self).__init__()\n",
    "        \n",
    "        self.n_states = n_dice\n",
    "        self.transition = torch.nn.init.normal(nn.Parameter(torch.randn(n_dice, n_dice + 1)), -1, 0.1)\n",
    "        self.loglikelihood = log_likelihood\n",
    "    \n",
    "\n",
    "    def to_scalar(self, var):\n",
    "#         print(\"to list: \", var.view(-1).data.tolist())\n",
    "        return var.view(-1).data.tolist()[0]\n",
    "\n",
    "\n",
    "    def argmax(self, vec):\n",
    "        # return the index of the max value for each row\n",
    "        _, idx = torch.max(vec, 1)\n",
    "        return self.to_scalar(idx)\n",
    "        \n",
    "    # Compute log sum exp in a numerically stable way for the forward algorithm\n",
    "    # Source: http://pytorch.org/tutorials/beginner/nlp/advanced_tutorial.html\n",
    "    def log_sum_exp(self, vec):\n",
    "#         print(\"self.argmax(vec): \", self.argmax(vec))\n",
    "        max_score = vec[0, self.argmax(vec)]\n",
    "#         print(\"max_score: \", max_score)\n",
    "#         print(\"max_score.view(1, -1): \", max_score.view(1, -1))\n",
    "        max_score_broadcast = max_score.view(1, -1).expand(1, vec.size()[1])\n",
    "#         print(\"vec.size(): \", vec.size())\n",
    "#         print(\"max_score_broadcast: \", max_score_broadcast)\n",
    "#         print(\"vec - max_score_broadcast: \", vec - max_score_broadcast)\n",
    "        return max_score + torch.log(torch.sum(torch.exp(vec - max_score_broadcast)))\n",
    "    \n",
    "    \n",
    "    def _data_to_likelihood(self, rolls):\n",
    "        \"\"\"Converts a numpy array of rolls (integers) to log-likelihood.\n",
    "        Input is one [1, n_rolls]\n",
    "        \"\"\"\n",
    "        return Variable(torch.FloatTensor(self.loglikelihood[rolls]), requires_grad=False)\n",
    "    \n",
    "    def _compute_likelihood_numerator(self, loglikelihoods, states):\n",
    "        # Let y be a tag sequence and x an input sequence of words\n",
    "        # p(y|x) = exp(Score(x, y))/sum_over_all_possible_sequences_(ie y')(exp(Score(x, y')))\n",
    "        # here returns Score(x, y) for a observed sequence of y\n",
    "        \n",
    "        \"\"\"Computes numerator of likelihood function for a given sequence.\n",
    "        \n",
    "        We'll iterate over the sequence of states and compute the sum \n",
    "        of the relevant transition cost with the log likelihood of the observed\n",
    "        roll. \n",
    "        Input:\n",
    "            loglikelihoods: torch Variable. Matrix of n_obs x n_states. \n",
    "                            i,j entry is loglikelihood of observing roll i given state j\n",
    "            states: sequence of labels \n",
    "        Output:\n",
    "            score: torch Variable. Score of assignment. \n",
    "        \"\"\"\n",
    "    \n",
    "        prev_state = self.n_states # initialized: n_states = n_dices \n",
    "        score = Variable(torch.Tensor([0]))\n",
    "        for index, state in enumerate(states): # a sequence of labels (i.e. [fair, fair, loaded, fair, ...])\n",
    "            # recall transition is parameter, shape [n_states, n_states+1], normal(-1, 0.1)\n",
    "            # loglikelihoods(emission matrix) passed by the argument\n",
    "            score += self.transition[state, prev_state] + loglikelihoods[index, state]\n",
    "            prev_state = state\n",
    "        return score\n",
    "    \n",
    "        \n",
    "    def _compute_likelihood_numerator(self, loglikelihoods, states):\n",
    "        \"\"\"Computes numerator of likelihood function for a given sequence.\n",
    "        \n",
    "        We'll iterate over the sequence of states and compute the sum \n",
    "        of the relevant transition cost with the log likelihood of the observed\n",
    "        roll. \n",
    "        Input:\n",
    "            loglikelihoods: torch Variable. Matrix of n_obs x n_states. \n",
    "                            i,j entry is loglikelihood of observing roll i given state j\n",
    "            states: sequence of labels\n",
    "        Output:\n",
    "            score: torch Variable. Score of assignment. \n",
    "        \"\"\"\n",
    "        prev_state = self.n_states\n",
    "        score = Variable(torch.Tensor([0]))\n",
    "        for index, state in enumerate(states):\n",
    "            score += self.transition[state, prev_state] + loglikelihoods[index, state]\n",
    "            prev_state = state\n",
    "        return score\n",
    "    \n",
    "    \n",
    "    def _compute_likelihood_denominator(self, loglikelihoods):\n",
    "        \"\"\"Implements the forward pass of the forward-backward algorithm.\n",
    "        \n",
    "        We loop over all possible states efficiently using the recursive\n",
    "        relationship: alpha_t(j) = \\sum_i alpha_{t-1}(i) * L(x_t | y_t) * C(y_t | y{t-1} = i)\n",
    "        Input:\n",
    "            loglikelihoods: torch Variable. Same input as _compute_likelihood_numerator.\n",
    "                            This algorithm efficiently loops over all possible state sequences\n",
    "                            so no other imput is needed.\n",
    "        Output:\n",
    "            torch Variable. \n",
    "        \"\"\"\n",
    "        # loglikelihood is like the emitt matrix w.r.t.observed values (a sentence):\n",
    "        # matrix of len(sentence) * tagset_size\n",
    "        \n",
    "        print(\"self.transition:\\n\", self.transition)\n",
    "        print(\"self.transition[:, self.n_states]:\\n\", self.transition[:, self.n_states])\n",
    "        print(\"loglikelihoods:\\n\", loglikelihoods)\n",
    "        print(\"loglikelihoods[0].view(1, -1):\\n\", loglikelihoods[0].view(1, -1))\n",
    "        \n",
    "        # Stores the current value of alpha at timestep t\n",
    "        prev_alpha = self.transition[:, self.n_states] + loglikelihoods[0].view(1, -1)\n",
    "        print(\"prev_alpha:\\n\", prev_alpha)\n",
    "        print(\"start loop:\\n\")\n",
    "        for roll in loglikelihoods[1:]:\n",
    "            print(\"current roll: \", roll)\n",
    "            alpha_t = []\n",
    "\n",
    "            # Loop over all possible states\n",
    "            for next_state in range(self.n_states):\n",
    "                print(\"current next_state: \", next_state)\n",
    "                print(\"self.transition[next_state,:self.n_states].view(1, -1):\\n\", \n",
    "                      self.transition[next_state,:self.n_states].view(1, -1))\n",
    "                print(\"roll[next_state].view(1, -1):\\n\", \n",
    "                      roll[next_state].view(1, -1))\n",
    "                print(\"roll[next_state].view(1, -1).expand(1, self.n_states):\\n\", \n",
    "                      roll[next_state].view(1, -1).expand(1, self.n_states))\n",
    "                # Compute all possible costs of transitioning to next_state\n",
    "                feature_function = self.transition[next_state,:self.n_states].view(1, -1) +\\\n",
    "                                   roll[next_state].view(1, -1).expand(1, self.n_states)\n",
    "                \n",
    "                print(\"feature_function: \", feature_function)\n",
    "                alpha_t_next_state = prev_alpha + feature_function\n",
    "                print(\"alpha_t_next_state:\\n\", alpha_t_next_state)\n",
    "                alpha_t.append(self.log_sum_exp(alpha_t_next_state))\n",
    "                print(\"log_sum_exp(alpha_t_next_state): \", self.log_sum_exp(alpha_t_next_state))\n",
    "                \n",
    "            prev_alpha = torch.cat(alpha_t).view(1, -1)\n",
    "            print(\"for current roll: prev_alpha\\n\", prev_alpha)\n",
    "        return self.log_sum_exp(prev_alpha)\n",
    "    \n",
    "    \n",
    "    def _viterbi_algorithm(self, loglikelihoods):\n",
    "        \"\"\"Implements Viterbi algorithm for finding most likely sequence of labels.\n",
    "        \n",
    "        Very similar to _compute_likelihood_denominator but now we take the maximum\n",
    "        over the previous states as opposed to the sum. \n",
    "        Input:\n",
    "            loglikelihoods: torch Variable. Same input as _compute_likelihood_denominator.\n",
    "        Output:\n",
    "            tuple. First entry is the most likely sequence of labels. Second is\n",
    "                   the loglikelihood of this sequence. \n",
    "        \"\"\"\n",
    "        \n",
    "        argmaxes = []\n",
    "\n",
    "        # prev_delta will store the current score of the sequence for each state\n",
    "        # prev_delta for the first observation\n",
    "        prev_delta = self.transition[:, self.n_states].contiguous().view(1, -1) +\\\n",
    "                      loglikelihoods[0].view(1, -1)\n",
    "\n",
    "        print(\"self.transition:\\n\", self.transition)\n",
    "        print(\"self.transition[:, self.n_states]:\\n\", self.transition[:, self.n_states])\n",
    "        print(\"loglikelihoods:\\n\", loglikelihoods)\n",
    "        print(\"loglikelihoods[0].view(1, -1):\\n\", loglikelihoods[0].view(1, -1))\n",
    "        print(\"prev_delta:\\n\", prev_delta)\n",
    "        print(\"start loop:\\n\")\n",
    "        \n",
    "        for roll in loglikelihoods[1:]:\n",
    "            print(\"current roll: \", roll)\n",
    "            local_argmaxes = []\n",
    "            next_delta = []\n",
    "            for next_state in range(self.n_states):\n",
    "                print(\"current next_state: \", next_state)\n",
    "                # we don't add emission here as it doesn't effect argmax\n",
    "                # since we look for most likely previous path that lead to this state, \n",
    "                # emssion score that relate this state to features at this timestep is irrelavent\n",
    "                feature_function = self.transition[next_state,:self.n_states].view(1, -1) + prev_delta\n",
    "                print(\"feature function:\\n\", feature_function)\n",
    "                # to figure out the best bp state (most likely previous state that leads to this state)\n",
    "                most_likely_state = self.argmax(feature_function)\n",
    "                local_argmaxes.append(most_likely_state)\n",
    "                score = feature_function[0][most_likely_state].view(1)\n",
    "                next_delta.append(score)\n",
    "                print(\"next_delta.append: \", score)\n",
    "                print(\"local_argmaxes.append: \", most_likely_state)\n",
    "                \n",
    "            # add emission part here to viterbi score for each state\n",
    "            prev_delta = (torch.cat(next_delta) + roll).view(1, -1)\n",
    "            print(\"prev_delta for this roll:\\n\", prev_delta)\n",
    "            argmaxes.append(local_argmaxes)\n",
    "            print(\"argmaxes.append after this roll: \", local_argmaxes)\n",
    "        \n",
    "        final_state = self.argmax(prev_delta)\n",
    "        final_score = prev_delta[0][final_state]\n",
    "        print(\"final_state: \", final_state)\n",
    "        print(\"final_score: \", final_score)\n",
    "        \n",
    "        path_list = [final_state]\n",
    "        # Backtrack through the argmaxes to find most likely state\n",
    "        for states in reversed(argmaxes):\n",
    "            final_state = states[final_state]\n",
    "            path_list.append(final_state)\n",
    "        print(\"best path before reverse: \", path_list)\n",
    "        path_list.reverse()\n",
    "        return final_score, path_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== a ==\n",
      " Variable containing:\n",
      " 0.4903  1.0318 -0.5989\n",
      " 1.6015 -1.0735 -1.2173\n",
      "[torch.FloatTensor of size 2x3]\n",
      "\n",
      "== crf.argmax ==\n",
      " 1\n",
      "== crf.log_sum_exp == \n",
      " Variable containing:\n",
      " 2.3596\n",
      "[torch.FloatTensor of size 1]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "a = torch.autograd.Variable(torch.randn(2, 3))\n",
    "print(\"== a ==\\n\", a)\n",
    "crf = CRF(2, log_likelihood)\n",
    "print(\"== crf.argmax ==\\n\", crf.argmax(a))\n",
    "print(\"== crf.log_sum_exp == \\n\", crf.log_sum_exp(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4,)\n",
      "Variable containing:\n",
      "-1.7918 -3.2189\n",
      "-1.7918 -3.2189\n",
      "-1.7918 -0.2231\n",
      "-1.7918 -3.2189\n",
      "[torch.FloatTensor of size 4x2]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rolls = np.array([1, 3, 5, 0])\n",
    "print(rolls.shape)\n",
    "print(crf._data_to_likelihood(rolls))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      "-12.5024\n",
      "[torch.FloatTensor of size 1]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "states = [0,0,1,0]\n",
    "numerator_score = crf._compute_likelihood_numerator(log_likelihood, states)\n",
    "print(numerator_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "self.transition:\n",
      " Parameter containing:\n",
      "-1.0461 -1.0099 -0.9527\n",
      "-0.8995 -1.0287 -1.1162\n",
      "[torch.FloatTensor of size 2x3]\n",
      "\n",
      "self.transition[:, self.n_states]:\n",
      " Variable containing:\n",
      "-0.9527\n",
      "-1.1162\n",
      "[torch.FloatTensor of size 2]\n",
      "\n",
      "loglikelihoods:\n",
      " Variable containing:\n",
      "-1.7918 -3.2189\n",
      "-1.7918 -3.2189\n",
      "-1.7918 -0.2231\n",
      "-1.7918 -3.2189\n",
      "[torch.FloatTensor of size 4x2]\n",
      "\n",
      "loglikelihoods[0].view(1, -1):\n",
      " Variable containing:\n",
      "-1.7918 -3.2189\n",
      "[torch.FloatTensor of size 1x2]\n",
      "\n",
      "prev_alpha:\n",
      " Variable containing:\n",
      "-2.7445 -4.3351\n",
      "[torch.FloatTensor of size 1x2]\n",
      "\n",
      "start loop:\n",
      "\n",
      "current roll:  Variable containing:\n",
      "-1.7918\n",
      "-3.2189\n",
      "[torch.FloatTensor of size 2]\n",
      "\n",
      "current next_state:  0\n",
      "self.transition[next_state,:self.n_states].view(1, -1):\n",
      " Variable containing:\n",
      "-1.0461 -1.0099\n",
      "[torch.FloatTensor of size 1x2]\n",
      "\n",
      "roll[next_state].view(1, -1):\n",
      " Variable containing:\n",
      "-1.7918\n",
      "[torch.FloatTensor of size 1x1]\n",
      "\n",
      "roll[next_state].view(1, -1).expand(1, self.n_states):\n",
      " Variable containing:\n",
      "-1.7918 -1.7918\n",
      "[torch.FloatTensor of size 1x2]\n",
      "\n",
      "feature_function:  Variable containing:\n",
      "-2.8378 -2.8017\n",
      "[torch.FloatTensor of size 1x2]\n",
      "\n",
      "alpha_t_next_state:\n",
      " Variable containing:\n",
      "-5.5823 -7.1367\n",
      "[torch.FloatTensor of size 1x2]\n",
      "\n",
      "log_sum_exp(alpha_t_next_state):  Variable containing:\n",
      "-5.3906\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "current next_state:  1\n",
      "self.transition[next_state,:self.n_states].view(1, -1):\n",
      " Variable containing:\n",
      "-0.8995 -1.0287\n",
      "[torch.FloatTensor of size 1x2]\n",
      "\n",
      "roll[next_state].view(1, -1):\n",
      " Variable containing:\n",
      "-3.2189\n",
      "[torch.FloatTensor of size 1x1]\n",
      "\n",
      "roll[next_state].view(1, -1).expand(1, self.n_states):\n",
      " Variable containing:\n",
      "-3.2189 -3.2189\n",
      "[torch.FloatTensor of size 1x2]\n",
      "\n",
      "feature_function:  Variable containing:\n",
      "-4.1184 -4.2476\n",
      "[torch.FloatTensor of size 1x2]\n",
      "\n",
      "alpha_t_next_state:\n",
      " Variable containing:\n",
      "-6.8629 -8.5827\n",
      "[torch.FloatTensor of size 1x2]\n",
      "\n",
      "log_sum_exp(alpha_t_next_state):  Variable containing:\n",
      "-6.6981\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "for current roll: prev_alpha\n",
      " Variable containing:\n",
      "-5.3906 -6.6981\n",
      "[torch.FloatTensor of size 1x2]\n",
      "\n",
      "current roll:  Variable containing:\n",
      "-1.7918\n",
      "-0.2231\n",
      "[torch.FloatTensor of size 2]\n",
      "\n",
      "current next_state:  0\n",
      "self.transition[next_state,:self.n_states].view(1, -1):\n",
      " Variable containing:\n",
      "-1.0461 -1.0099\n",
      "[torch.FloatTensor of size 1x2]\n",
      "\n",
      "roll[next_state].view(1, -1):\n",
      " Variable containing:\n",
      "-1.7918\n",
      "[torch.FloatTensor of size 1x1]\n",
      "\n",
      "roll[next_state].view(1, -1).expand(1, self.n_states):\n",
      " Variable containing:\n",
      "-1.7918 -1.7918\n",
      "[torch.FloatTensor of size 1x2]\n",
      "\n",
      "feature_function:  Variable containing:\n",
      "-2.8378 -2.8017\n",
      "[torch.FloatTensor of size 1x2]\n",
      "\n",
      "alpha_t_next_state:\n",
      " Variable containing:\n",
      "-8.2284 -9.4998\n",
      "[torch.FloatTensor of size 1x2]\n",
      "\n",
      "log_sum_exp(alpha_t_next_state):  Variable containing:\n",
      "-7.9812\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "current next_state:  1\n",
      "self.transition[next_state,:self.n_states].view(1, -1):\n",
      " Variable containing:\n",
      "-0.8995 -1.0287\n",
      "[torch.FloatTensor of size 1x2]\n",
      "\n",
      "roll[next_state].view(1, -1):\n",
      " Variable containing:\n",
      "-0.2231\n",
      "[torch.FloatTensor of size 1x1]\n",
      "\n",
      "roll[next_state].view(1, -1).expand(1, self.n_states):\n",
      " Variable containing:\n",
      "-0.2231 -0.2231\n",
      "[torch.FloatTensor of size 1x2]\n",
      "\n",
      "feature_function:  Variable containing:\n",
      "-1.1226 -1.2519\n",
      "[torch.FloatTensor of size 1x2]\n",
      "\n",
      "alpha_t_next_state:\n",
      " Variable containing:\n",
      "-6.5133 -7.9500\n",
      "[torch.FloatTensor of size 1x2]\n",
      "\n",
      "log_sum_exp(alpha_t_next_state):  Variable containing:\n",
      "-6.3000\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "for current roll: prev_alpha\n",
      " Variable containing:\n",
      "-7.9812 -6.3000\n",
      "[torch.FloatTensor of size 1x2]\n",
      "\n",
      "current roll:  Variable containing:\n",
      "-1.7918\n",
      "-3.2189\n",
      "[torch.FloatTensor of size 2]\n",
      "\n",
      "current next_state:  0\n",
      "self.transition[next_state,:self.n_states].view(1, -1):\n",
      " Variable containing:\n",
      "-1.0461 -1.0099\n",
      "[torch.FloatTensor of size 1x2]\n",
      "\n",
      "roll[next_state].view(1, -1):\n",
      " Variable containing:\n",
      "-1.7918\n",
      "[torch.FloatTensor of size 1x1]\n",
      "\n",
      "roll[next_state].view(1, -1).expand(1, self.n_states):\n",
      " Variable containing:\n",
      "-1.7918 -1.7918\n",
      "[torch.FloatTensor of size 1x2]\n",
      "\n",
      "feature_function:  Variable containing:\n",
      "-2.8378 -2.8017\n",
      "[torch.FloatTensor of size 1x2]\n",
      "\n",
      "alpha_t_next_state:\n",
      " Variable containing:\n",
      "-10.8191  -9.1017\n",
      "[torch.FloatTensor of size 1x2]\n",
      "\n",
      "log_sum_exp(alpha_t_next_state):  Variable containing:\n",
      "-8.9365\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "current next_state:  1\n",
      "self.transition[next_state,:self.n_states].view(1, -1):\n",
      " Variable containing:\n",
      "-0.8995 -1.0287\n",
      "[torch.FloatTensor of size 1x2]\n",
      "\n",
      "roll[next_state].view(1, -1):\n",
      " Variable containing:\n",
      "-3.2189\n",
      "[torch.FloatTensor of size 1x1]\n",
      "\n",
      "roll[next_state].view(1, -1).expand(1, self.n_states):\n",
      " Variable containing:\n",
      "-3.2189 -3.2189\n",
      "[torch.FloatTensor of size 1x2]\n",
      "\n",
      "feature_function:  Variable containing:\n",
      "-4.1184 -4.2476\n",
      "[torch.FloatTensor of size 1x2]\n",
      "\n",
      "alpha_t_next_state:\n",
      " Variable containing:\n",
      "-12.0996 -10.5476\n",
      "[torch.FloatTensor of size 1x2]\n",
      "\n",
      "log_sum_exp(alpha_t_next_state):  Variable containing:\n",
      "-10.3555\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "for current roll: prev_alpha\n",
      " Variable containing:\n",
      " -8.9365 -10.3555\n",
      "[torch.FloatTensor of size 1x2]\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       "-8.7198\n",
       "[torch.FloatTensor of size 1]"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_likelihoods = crf._data_to_likelihood(rolls)\n",
    "crf._compute_likelihood_denominator(log_likelihoods)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "self.transition:\n",
      " Parameter containing:\n",
      "-1.0461 -1.0099 -0.9527\n",
      "-0.8995 -1.0287 -1.1162\n",
      "[torch.FloatTensor of size 2x3]\n",
      "\n",
      "self.transition[:, self.n_states]:\n",
      " Variable containing:\n",
      "-0.9527\n",
      "-1.1162\n",
      "[torch.FloatTensor of size 2]\n",
      "\n",
      "loglikelihoods:\n",
      " Variable containing:\n",
      "-1.7918 -3.2189\n",
      "-1.7918 -3.2189\n",
      "-1.7918 -0.2231\n",
      "-1.7918 -3.2189\n",
      "[torch.FloatTensor of size 4x2]\n",
      "\n",
      "loglikelihoods[0].view(1, -1):\n",
      " Variable containing:\n",
      "-1.7918 -3.2189\n",
      "[torch.FloatTensor of size 1x2]\n",
      "\n",
      "prev_delta:\n",
      " Variable containing:\n",
      "-2.7445 -4.3351\n",
      "[torch.FloatTensor of size 1x2]\n",
      "\n",
      "start loop:\n",
      "\n",
      "current roll:  Variable containing:\n",
      "-1.7918\n",
      "-3.2189\n",
      "[torch.FloatTensor of size 2]\n",
      "\n",
      "current next_state:  0\n",
      "feature function:\n",
      " Variable containing:\n",
      "-3.7906 -5.3450\n",
      "[torch.FloatTensor of size 1x2]\n",
      "\n",
      "next_delta.append:  Variable containing:\n",
      "-3.7906\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "local_argmaxes.append:  0\n",
      "current next_state:  1\n",
      "feature function:\n",
      " Variable containing:\n",
      "-3.6440 -5.3638\n",
      "[torch.FloatTensor of size 1x2]\n",
      "\n",
      "next_delta.append:  Variable containing:\n",
      "-3.6440\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "local_argmaxes.append:  0\n",
      "prev_delta for this roll:\n",
      " Variable containing:\n",
      "-5.5823 -6.8629\n",
      "[torch.FloatTensor of size 1x2]\n",
      "\n",
      "argmaxes.append after this roll:  [0, 0]\n",
      "current roll:  Variable containing:\n",
      "-1.7918\n",
      "-0.2231\n",
      "[torch.FloatTensor of size 2]\n",
      "\n",
      "current next_state:  0\n",
      "feature function:\n",
      " Variable containing:\n",
      "-6.6284 -7.8728\n",
      "[torch.FloatTensor of size 1x2]\n",
      "\n",
      "next_delta.append:  Variable containing:\n",
      "-6.6284\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "local_argmaxes.append:  0\n",
      "current next_state:  1\n",
      "feature function:\n",
      " Variable containing:\n",
      "-6.4818 -7.8916\n",
      "[torch.FloatTensor of size 1x2]\n",
      "\n",
      "next_delta.append:  Variable containing:\n",
      "-6.4818\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "local_argmaxes.append:  0\n",
      "prev_delta for this roll:\n",
      " Variable containing:\n",
      "-8.4201 -6.7050\n",
      "[torch.FloatTensor of size 1x2]\n",
      "\n",
      "argmaxes.append after this roll:  [0, 0]\n",
      "current roll:  Variable containing:\n",
      "-1.7918\n",
      "-3.2189\n",
      "[torch.FloatTensor of size 2]\n",
      "\n",
      "current next_state:  0\n",
      "feature function:\n",
      " Variable containing:\n",
      "-9.4662 -7.7149\n",
      "[torch.FloatTensor of size 1x2]\n",
      "\n",
      "next_delta.append:  Variable containing:\n",
      "-7.7149\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "local_argmaxes.append:  1\n",
      "current next_state:  1\n",
      "feature function:\n",
      " Variable containing:\n",
      "-9.3197 -7.7337\n",
      "[torch.FloatTensor of size 1x2]\n",
      "\n",
      "next_delta.append:  Variable containing:\n",
      "-7.7337\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "local_argmaxes.append:  1\n",
      "prev_delta for this roll:\n",
      " Variable containing:\n",
      " -9.5066 -10.9525\n",
      "[torch.FloatTensor of size 1x2]\n",
      "\n",
      "argmaxes.append after this roll:  [1, 1]\n",
      "final_state:  0\n",
      "final_score:  Variable containing:\n",
      "-9.5066\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "best path before reverse:  [0, 1, 0, 0]\n",
      "==final score:  Variable containing:\n",
      "-9.5066\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "==best path:  [0, 0, 1, 0]\n"
     ]
    }
   ],
   "source": [
    "score, best_path = crf._viterbi_algorithm(log_likelihoods)\n",
    "print(\"==final score: \", score)\n",
    "print(\"==best path: \", best_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:anaconda]",
   "language": "python",
   "name": "conda-env-anaconda-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
