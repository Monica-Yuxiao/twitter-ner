{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch.optim import Adam\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CRF(nn.Module):\n",
    "    \n",
    "    def __init__(self, n_dice, log_likelihood):\n",
    "        super(CRF, self).__init__()\n",
    "        \n",
    "        self.n_states = n_dice\n",
    "        self.transition = torch.nn.init.normal(nn.Parameter(torch.randn(n_dice, n_dice + 1)), -1, 0.1)\n",
    "        self.loglikelihood = log_likelihood\n",
    "    \n",
    "\n",
    "    def to_scalar(self, var):\n",
    "        return var.view(-1).data.tolist()[0]\n",
    "\n",
    "\n",
    "    def argmax(self, vec):\n",
    "        _, idx = torch.max(vec, 1)\n",
    "        return self.to_scalar(idx)\n",
    "        \n",
    "    # numerically stable log sum exp\n",
    "    # Source: http://pytorch.org/tutorials/beginner/nlp/advanced_tutorial.html\n",
    "    def log_sum_exp(self, vec):\n",
    "        max_score = vec[0, self.argmax(vec)]\n",
    "        max_score_broadcast = max_score.view(1, -1).expand(1, vec.size()[1])\n",
    "        return max_score + \\\n",
    "               torch.log(torch.sum(torch.exp(vec - max_score_broadcast)))\n",
    "    \n",
    "    \n",
    "    def _data_to_likelihood(self, rolls):\n",
    "        \"\"\"Converts a numpy array of rolls (integers) to log-likelihood.\n",
    "        Input is one [1, n_rolls]\n",
    "        \"\"\"\n",
    "        return Variable(torch.FloatTensor(self.loglikelihood[rolls]), requires_grad=False)\n",
    "        \n",
    "    \n",
    "    def _compute_likelihood_numerator(self, loglikelihoods, states):\n",
    "        \"\"\"Computes numerator of likelihood function for a given sequence.\n",
    "        \n",
    "        We'll iterate over the sequence of states and compute the sum \n",
    "        of the relevant transition cost with the log likelihood of the observed\n",
    "        roll. \n",
    "        Input:\n",
    "            loglikelihoods: torch Variable. Matrix of n_obs x n_states. \n",
    "                            i,j entry is loglikelihood of observing roll i given state j\n",
    "            states: sequence of labels\n",
    "        Output:\n",
    "            score: torch Variable. Score of assignment. \n",
    "        \"\"\"\n",
    "        prev_state = self.n_states\n",
    "        score = Variable(torch.Tensor([0]))\n",
    "        for index, state in enumerate(states):\n",
    "            score += self.transition[state, prev_state] + loglikelihoods[index, state]\n",
    "            prev_state = state\n",
    "        return score\n",
    "    \n",
    "    def _compute_likelihood_denominator(self, loglikelihoods):\n",
    "        \"\"\"Implements the forward pass of the forward-backward algorithm.\n",
    "        \n",
    "        We loop over all possible states efficiently using the recursive\n",
    "        relationship: alpha_t(j) = \\sum_i alpha_{t-1}(i) * L(x_t | y_t) * C(y_t | y{t-1} = i)\n",
    "        Input:\n",
    "            loglikelihoods: torch Variable. Same input as _compute_likelihood_numerator.\n",
    "                            This algorithm efficiently loops over all possible state sequences\n",
    "                            so no other imput is needed.\n",
    "        Output:\n",
    "            torch Variable. \n",
    "        \"\"\"\n",
    "\n",
    "        # Stores the current value of alpha at timestep t\n",
    "        prev_alpha = self.transition[:, self.n_states] + loglikelihoods[0].view(1, -1)\n",
    "\n",
    "        for roll in loglikelihoods[1:]:\n",
    "            alpha_t = []\n",
    "\n",
    "            # Loop over all possible states\n",
    "            for next_state in range(self.n_states):\n",
    "                \n",
    "                # Compute all possible costs of transitioning to next_state\n",
    "                feature_function = self.transition[next_state,:self.n_states].view(1, -1) +\\\n",
    "                                   roll[next_state].view(1, -1).expand(1, self.n_states)\n",
    "\n",
    "                alpha_t_next_state = prev_alpha + feature_function\n",
    "                alpha_t.append(self.log_sum_exp(alpha_t_next_state))\n",
    "            prev_alpha = torch.cat(alpha_t).view(1, -1)\n",
    "        return self.log_sum_exp(prev_alpha)\n",
    "    \n",
    "    def _viterbi_algorithm(self, loglikelihoods):\n",
    "        \"\"\"Implements Viterbi algorithm for finding most likely sequence of labels.\n",
    "        \n",
    "        Very similar to _compute_likelihood_denominator but now we take the maximum\n",
    "        over the previous states as opposed to the sum. \n",
    "        Input:\n",
    "            loglikelihoods: torch Variable. Same input as _compute_likelihood_denominator.\n",
    "        Output:\n",
    "            tuple. First entry is the most likely sequence of labels. Second is\n",
    "                   the loglikelihood of this sequence. \n",
    "        \"\"\"\n",
    "\n",
    "        argmaxes = []\n",
    "\n",
    "        # prev_delta will store the current score of the sequence for each state\n",
    "        prev_delta = self.transition[:, self.n_states].contiguous().view(1, -1) +\\\n",
    "                      loglikelihoods[0].view(1, -1)\n",
    "\n",
    "        for roll in loglikelihoods[1:]:\n",
    "            local_argmaxes = []\n",
    "            next_delta = []\n",
    "            for next_state in range(self.n_states):\n",
    "                feature_function = self.transition[next_state,:self.n_states].view(1, -1) +\\\n",
    "                                   roll.view(1, -1) +\\\n",
    "                                   prev_delta\n",
    "                most_likely_state = self.argmax(feature_function)\n",
    "                score = feature_function[0][most_likely_state]\n",
    "                next_delta.append(score)\n",
    "                local_argmaxes.append(most_likely_state)\n",
    "            prev_delta = torch.cat(next_delta).view(1, -1)\n",
    "            argmaxes.append(local_argmaxes)\n",
    "        \n",
    "        final_state = self.argmax(prev_delta)\n",
    "        final_score = prev_delta[0][final_state]\n",
    "        path_list = [final_state]\n",
    "\n",
    "        # Backtrack through the argmaxes to find most likely state\n",
    "        for states in reversed(argmaxes):\n",
    "            final_state = states[final_state]\n",
    "            path_list.append(final_state)\n",
    "        \n",
    "        return np.array(path_list), final_score\n",
    "        \n",
    "    def neg_log_likelihood(self, rolls, states):\n",
    "        \"\"\"Compute neg log-likelihood for a given sequence.\n",
    "        \n",
    "        Input: \n",
    "            rolls: numpy array, dim [1, n_rolls]. Integer 0-5 showing value on dice.\n",
    "            states: numpy array, dim [1, n_rolls]. Integer 0, 1. 0 if dice is fair.\n",
    "        \"\"\"\n",
    "        loglikelihoods = self._data_to_likelihood(rolls)\n",
    "        states = torch.LongTensor(states)\n",
    "        \n",
    "        sequence_loglik = self._compute_likelihood_numerator(loglikelihoods, states)\n",
    "        denominator = self._compute_likelihood_denominator(loglikelihoods)\n",
    "        return denominator - sequence_loglik\n",
    "               \n",
    "    \n",
    "    def forward(self, rolls):\n",
    "        loglikelihoods = self._data_to_likelihood(rolls)\n",
    "        return self._viterbi_algorithm(loglikelihoods)\n",
    "                \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:anaconda]",
   "language": "python",
   "name": "conda-env-anaconda-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
