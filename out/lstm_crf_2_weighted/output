/anaconda/bin/python3.5 /Users/yuxiaozhang/Documents/19spring/cs273/twitter-ner/train_2.py
how many examples:  1914
first example:
 (['@paulwalk', 'It', "'s", 'the', 'view', 'from', 'where', 'I', "'m", 'living', 'for', 'two', 'weeks', '.', 'Empire', 'State', 'Building', '=', 'ESB', '.', 'Pretty', 'bad', 'storm', 'here', 'last', 'evening', '.'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-facility', 'I-facility', 'I-facility', 'O', 'B-facility', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'])
last example:
 (['Avast', 'AntiVirus', '4', '8', 'PROFIONAL', 'Full', 'Life', 'Time', 'Key', ':', 'Avast', 'AntiVirus', '4.8', 'PROFESIONAL', 'Full', 'with', 'Life', 'Time'], ['B-product', 'I-product', 'I-product', 'I-product', 'I-product', 'O', 'O', 'O', 'O', 'O', 'B-product', 'I-product', 'I-product', 'I-product', 'O', 'O', 'O', 'O'])
feature for 1st example:
 [[0, 1], [1, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [1, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [1, 0], [1, 0], [1, 0], [0, 0], [1, 0], [0, 0], [1, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0]]
feature for last example:
 [[1, 0], [1, 0], [0, 0], [0, 0], [1, 0], [1, 0], [1, 0], [1, 0], [1, 0], [0, 1], [1, 0], [1, 0], [0, 0], [1, 0], [1, 0], [0, 0], [1, 0], [1, 0]]
== vocab size:  8910
== how many rare word:  6199
== word_to_ix size:  2712
how many additional features:  2
epoch 0
epoch 0: train_loss = 2.975518 val_loss = 2.568609
epoch 1
epoch 1: train_loss = 2.742715 val_loss = 2.457925
epoch 2
epoch 2: train_loss = 2.641744 val_loss = 2.408255
epoch 3
epoch 3: train_loss = 2.580125 val_loss = 2.388939
epoch 4
epoch 4: train_loss = 2.537381 val_loss = 2.352208
epoch 5
epoch 5: train_loss = 2.502274 val_loss = 2.339725
epoch 6
epoch 6: train_loss = 2.475756 val_loss = 2.283847
epoch 7
epoch 7: train_loss = 2.452231 val_loss = 2.301993
epoch 8
epoch 8: train_loss = 2.430065 val_loss = 2.365167
epoch 9
epoch 9: train_loss = 2.414502 val_loss = 2.289472
epoch 10
epoch 10: train_loss = 2.398342 val_loss = 2.274579
epoch 11
epoch 11: train_loss = 2.383466 val_loss = 2.289451
epoch 12
epoch 12: train_loss = 2.370646 val_loss = 2.263127
epoch 13
epoch 13: train_loss = 2.357817 val_loss = 2.206567
epoch 14
epoch 14: train_loss = 2.345898 val_loss = 2.257171
epoch 15
epoch 15: train_loss = 2.334118 val_loss = 2.253928
epoch 16
epoch 16: train_loss = 2.323859 val_loss = 2.227833
epoch 17
epoch 17: train_loss = 2.314506 val_loss = 2.255163
epoch 18
epoch 18: train_loss = 2.305098 val_loss = 2.257075
epoch 19
epoch 19: train_loss = 2.295113 val_loss = 2.221588
epoch 20
epoch 20: train_loss = 2.286527 val_loss = 2.284325
epoch 21
epoch 21: train_loss = 2.276571 val_loss = 2.266614
epoch 22
epoch 22: train_loss = 2.268368 val_loss = 2.178084
epoch 23
epoch 23: train_loss = 2.260733 val_loss = 2.219077
epoch 24
epoch 24: train_loss = 2.253219 val_loss = 2.228521
epoch 25
epoch 25: train_loss = 2.244125 val_loss = 2.199784
epoch 26
epoch 26: train_loss = 2.237458 val_loss = 2.231174
epoch 27
epoch 27: train_loss = 2.229187 val_loss = 2.179044
epoch 28
epoch 28: train_loss = 2.222195 val_loss = 2.169598
epoch 29
epoch 29: train_loss = 2.214442 val_loss = 2.177866
epoch 30
epoch 30: train_loss = 2.207524 val_loss = 2.254232
epoch 31
epoch 31: train_loss = 2.199989 val_loss = 2.195347
epoch 32
epoch 32: train_loss = 2.193005 val_loss = 2.205443
epoch 33
epoch 33: train_loss = 2.185495 val_loss = 2.199175
epoch 34
epoch 34: train_loss = 2.178239 val_loss = 2.174112
epoch 35
epoch 35: train_loss = 2.171960 val_loss = 2.194882
epoch 36
epoch 36: train_loss = 2.164176 val_loss = 2.154548
epoch 37
epoch 37: train_loss = 2.157280 val_loss = 2.192630
epoch 38
epoch 38: train_loss = 2.151118 val_loss = 2.153585
epoch 39
epoch 39: train_loss = 2.145115 val_loss = 2.139173
epoch 40
epoch 40: train_loss = 2.138690 val_loss = 2.174194
epoch 41
epoch 41: train_loss = 2.132539 val_loss = 2.111344
epoch 42
epoch 42: train_loss = 2.126281 val_loss = 2.133680
epoch 43
epoch 43: train_loss = 2.120532 val_loss = 2.169409
epoch 44
epoch 44: train_loss = 2.114516 val_loss = 2.141235
epoch 45
epoch 45: train_loss = 2.108950 val_loss = 2.076973
epoch 46
epoch 46: train_loss = 2.102889 val_loss = 2.176381
epoch 47
epoch 47: train_loss = 2.097510 val_loss = 2.085324
epoch 48
epoch 48: train_loss = 2.091992 val_loss = 2.162681
epoch 49
epoch 49: train_loss = 2.086425 val_loss = 2.048502
epoch 50
epoch 50: train_loss = 2.080364 val_loss = 2.090309
epoch 51
epoch 51: train_loss = 2.074615 val_loss = 2.119973
epoch 52
epoch 52: train_loss = 2.068997 val_loss = 2.143066
epoch 53
epoch 53: train_loss = 2.062964 val_loss = 2.101170
epoch 54
epoch 54: train_loss = 2.057803 val_loss = 2.110965
epoch 55
epoch 55: train_loss = 2.052630 val_loss = 2.036338
epoch 56
epoch 56: train_loss = 2.046748 val_loss = 2.104333
epoch 57
epoch 57: train_loss = 2.041442 val_loss = 2.089041
epoch 58
epoch 58: train_loss = 2.035974 val_loss = 2.029294
epoch 59
epoch 59: train_loss = 2.030878 val_loss = 2.126156
epoch 60
epoch 60: train_loss = 2.025822 val_loss = 2.017990
epoch 61
epoch 61: train_loss = 2.020487 val_loss = 2.131430
epoch 62
epoch 62: train_loss = 2.015970 val_loss = 2.029481
epoch 63
epoch 63: train_loss = 2.011106 val_loss = 2.149860
epoch 64
epoch 64: train_loss = 2.005957 val_loss = 2.115893
epoch 65
epoch 65: train_loss = 2.000813 val_loss = 2.120544
epoch 66
epoch 66: train_loss = 1.995236 val_loss = 2.069273
epoch 67
epoch 67: train_loss = 1.990064 val_loss = 2.133887
epoch 68
epoch 68: train_loss = 1.985014 val_loss = 2.217764
epoch 69
epoch 69: train_loss = 1.980525 val_loss = 2.133437
epoch 70
epoch 70: train_loss = 1.975368 val_loss = 2.051718
epoch 71
epoch 71: train_loss = 1.970718 val_loss = 1.984173
epoch 72
epoch 72: train_loss = 1.965994 val_loss = 2.069739
epoch 73
epoch 73: train_loss = 1.961059 val_loss = 2.121919
epoch 74
epoch 74: train_loss = 1.956559 val_loss = 2.058750
epoch 75
epoch 75: train_loss = 1.952067 val_loss = 2.042306
epoch 76
epoch 76: train_loss = 1.947145 val_loss = 2.140877
epoch 77
epoch 77: train_loss = 1.942778 val_loss = 2.148077
epoch 78
epoch 78: train_loss = 1.938410 val_loss = 2.150338
epoch 79
epoch 79: train_loss = 1.934130 val_loss = 2.120295
epoch 80
epoch 80: train_loss = 1.929542 val_loss = 2.094712
epoch 81
epoch 81: train_loss = 1.925645 val_loss = 2.127558
epoch 82
epoch 82: train_loss = 1.921014 val_loss = 1.985739
epoch 83
epoch 83: train_loss = 1.916594 val_loss = 2.031233
epoch 84
epoch 84: train_loss = 1.912528 val_loss = 2.094105
epoch 85
epoch 85: train_loss = 1.908545 val_loss = 2.046586
epoch 86
epoch 86: train_loss = 1.904045 val_loss = 2.032677
epoch 87
epoch 87: train_loss = 1.899691 val_loss = 2.060207
epoch 88
epoch 88: train_loss = 1.895281 val_loss = 2.079627
epoch 89
epoch 89: train_loss = 1.890827 val_loss = 2.087922
epoch 90
epoch 90: train_loss = 1.886596 val_loss = 2.143177
epoch 91
epoch 91: train_loss = 1.882431 val_loss = 2.123473
epoch 92
epoch 92: train_loss = 1.878352 val_loss = 2.075662
epoch 93
epoch 93: train_loss = 1.874376 val_loss = 2.044026
epoch 94
epoch 94: train_loss = 1.870293 val_loss = 1.991425
epoch 95
epoch 95: train_loss = 1.866512 val_loss = 1.882037
epoch 96
epoch 96: train_loss = 1.862081 val_loss = 2.154166
epoch 97
epoch 97: train_loss = 1.858135 val_loss = 2.092741
epoch 98
epoch 98: train_loss = 1.853989 val_loss = 1.974488
epoch 99
epoch 99: train_loss = 1.849725 val_loss = 2.062418
epoch 100
epoch 100: train_loss = 1.845958 val_loss = 2.069335
epoch 101
epoch 101: train_loss = 1.842150 val_loss = 2.080442
epoch 102
epoch 102: train_loss = 1.838295 val_loss = 2.063964
epoch 103
epoch 103: train_loss = 1.834808 val_loss = 2.015520
epoch 104
epoch 104: train_loss = 1.830903 val_loss = 2.116380
epoch 105
epoch 105: train_loss = 1.826889 val_loss = 2.147382
epoch 106
epoch 106: train_loss = 1.823133 val_loss = 2.002440
epoch 107
epoch 107: train_loss = 1.819415 val_loss = 2.112700
epoch 108
epoch 108: train_loss = 1.815719 val_loss = 2.076241
epoch 109
epoch 109: train_loss = 1.812121 val_loss = 1.994395
epoch 110
epoch 110: train_loss = 1.808170 val_loss = 2.079215
epoch 111
epoch 111: train_loss = 1.804469 val_loss = 2.080746
epoch 112
epoch 112: train_loss = 1.800655 val_loss = 2.127805
epoch 113
epoch 113: train_loss = 1.797220 val_loss = 2.139835
epoch 114
epoch 114: train_loss = 1.793166 val_loss = 2.169836
epoch 115
epoch 115: train_loss = 1.789361 val_loss = 2.192177
epoch 116
epoch 116: train_loss = 1.785975 val_loss = 2.128775
epoch 117
epoch 117: train_loss = 1.782531 val_loss = 2.235990
epoch 118
epoch 118: train_loss = 1.778843 val_loss = 2.021838
epoch 119
epoch 119: train_loss = 1.775393 val_loss = 2.060072
epoch 120
epoch 120: train_loss = 1.771865 val_loss = 2.085743
epoch 121
epoch 121: train_loss = 1.768054 val_loss = 2.180844
epoch 122
epoch 122: train_loss = 1.764652 val_loss = 2.150164
epoch 123
epoch 123: train_loss = 1.761088 val_loss = 2.175148
epoch 124
epoch 124: train_loss = 1.757704 val_loss = 2.033456
epoch 125
epoch 125: train_loss = 1.754401 val_loss = 2.136514
epoch 126
epoch 126: train_loss = 1.750842 val_loss = 2.179175
epoch 127
epoch 127: train_loss = 1.747389 val_loss = 2.096302
epoch 128
epoch 128: train_loss = 1.743802 val_loss = 2.233401
epoch 129
epoch 129: train_loss = 1.740368 val_loss = 2.156475
epoch 130
epoch 130: train_loss = 1.737111 val_loss = 2.174495
epoch 131
epoch 131: train_loss = 1.733909 val_loss = 1.999491
epoch 132
epoch 132: train_loss = 1.730735 val_loss = 2.153194
epoch 133
epoch 133: train_loss = 1.727415 val_loss = 2.092470
epoch 134
epoch 134: train_loss = 1.724279 val_loss = 2.051816
epoch 135
epoch 135: train_loss = 1.720923 val_loss = 2.080216
epoch 136
epoch 136: train_loss = 1.717643 val_loss = 2.153497
epoch 137
epoch 137: train_loss = 1.714427 val_loss = 2.122436
epoch 138
epoch 138: train_loss = 1.711345 val_loss = 2.006381
epoch 139
epoch 139: train_loss = 1.708094 val_loss = 2.076267
epoch 140
epoch 140: train_loss = 1.704683 val_loss = 2.122425
epoch 141
epoch 141: train_loss = 1.701547 val_loss = 2.089668
epoch 142
epoch 142: train_loss = 1.698604 val_loss = 2.162214
epoch 143
epoch 143: train_loss = 1.695736 val_loss = 2.141523
epoch 144
epoch 144: train_loss = 1.692688 val_loss = 2.137164
epoch 145
epoch 145: train_loss = 1.689502 val_loss = 2.146860
epoch 146
epoch 146: train_loss = 1.686414 val_loss = 2.185859
epoch 147
epoch 147: train_loss = 1.683565 val_loss = 2.182265
epoch 148
epoch 148: train_loss = 1.680393 val_loss = 2.279764
epoch 149
epoch 149: train_loss = 1.677546 val_loss = 2.136175
epoch 150
epoch 150: train_loss = 1.674595 val_loss = 2.175184
epoch 151
epoch 151: train_loss = 1.671584 val_loss = 2.264729
epoch 152
epoch 152: train_loss = 1.668688 val_loss = 2.201284
epoch 153
epoch 153: train_loss = 1.665827 val_loss = 2.160368
epoch 154
epoch 154: train_loss = 1.662990 val_loss = 2.121537
epoch 155
epoch 155: train_loss = 1.660050 val_loss = 2.226729
epoch 156
epoch 156: train_loss = 1.657128 val_loss = 2.179860
epoch 157
epoch 157: train_loss = 1.654315 val_loss = 2.119107
epoch 158
epoch 158: train_loss = 1.651632 val_loss = 2.080147
epoch 159
epoch 159: train_loss = 1.648611 val_loss = 2.126757
epoch 160
epoch 160: train_loss = 1.645766 val_loss = 2.073181
epoch 161
epoch 161: train_loss = 1.643071 val_loss = 2.118740
epoch 162
epoch 162: train_loss = 1.640241 val_loss = 2.150090
epoch 163
epoch 163: train_loss = 1.637532 val_loss = 2.144649
epoch 164
epoch 164: train_loss = 1.634725 val_loss = 2.177199
epoch 165
epoch 165: train_loss = 1.632095 val_loss = 2.143836
epoch 166
epoch 166: train_loss = 1.629266 val_loss = 2.229391
epoch 167
epoch 167: train_loss = 1.626635 val_loss = 2.121704
epoch 168
epoch 168: train_loss = 1.624068 val_loss = 2.115367
epoch 169
epoch 169: train_loss = 1.621491 val_loss = 2.208960
epoch 170
epoch 170: train_loss = 1.618889 val_loss = 1.988328
epoch 171
epoch 171: train_loss = 1.616331 val_loss = 2.151718
epoch 172
epoch 172: train_loss = 1.613534 val_loss = 2.332262
epoch 173
epoch 173: train_loss = 1.611023 val_loss = 2.118878
epoch 174
epoch 174: train_loss = 1.608294 val_loss = 2.232319
epoch 175
epoch 175: train_loss = 1.605651 val_loss = 2.166146
epoch 176
epoch 176: train_loss = 1.602932 val_loss = 2.232834
epoch 177
epoch 177: train_loss = 1.600276 val_loss = 2.195825
epoch 178
epoch 178: train_loss = 1.597492 val_loss = 2.219150
epoch 179
epoch 179: train_loss = 1.594845 val_loss = 2.210268
epoch 180
epoch 180: train_loss = 1.592289 val_loss = 2.195265
epoch 181
epoch 181: train_loss = 1.589843 val_loss = 2.203241
epoch 182
epoch 182: train_loss = 1.587336 val_loss = 2.148603
epoch 183
epoch 183: train_loss = 1.584598 val_loss = 2.359459
epoch 184
epoch 184: train_loss = 1.582147 val_loss = 2.252229
epoch 185
epoch 185: train_loss = 1.579654 val_loss = 2.189041
epoch 186
epoch 186: train_loss = 1.577338 val_loss = 2.211715
epoch 187
epoch 187: train_loss = 1.574619 val_loss = 2.194809
epoch 188
epoch 188: train_loss = 1.572160 val_loss = 2.288848
epoch 189
epoch 189: train_loss = 1.569662 val_loss = 2.301960
epoch 190
epoch 190: train_loss = 1.567197 val_loss = 2.248383
epoch 191
epoch 191: train_loss = 1.564687 val_loss = 2.090464
epoch 192
epoch 192: train_loss = 1.562368 val_loss = 2.295426
epoch 193
epoch 193: train_loss = 1.559818 val_loss = 2.173712
epoch 194
epoch 194: train_loss = 1.557500 val_loss = 2.226587
epoch 195
epoch 195: train_loss = 1.555114 val_loss = 2.204016
epoch 196
epoch 196: train_loss = 1.552680 val_loss = 2.357741
epoch 197
epoch 197: train_loss = 1.550186 val_loss = 2.333144
epoch 198
epoch 198: train_loss = 1.547857 val_loss = 2.171198
epoch 199
epoch 199: train_loss = 1.545538 val_loss = 2.095429
0 tags: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 1, 0, 2, 0, 0, 0, 0, 0,
        0, 0, 0])

After training: (tensor(139.2068), [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0])

1 tags: tensor([2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])

After training: (tensor(72.2007), [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])

2 tags: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0])

After training: (tensor(49.8269), [0, 0, 0, 0, 0, 0, 0, 0, 0])

3 tags: tensor([2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])

After training: (tensor(81.9010), [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])

4 tags: tensor([0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])

After training: (tensor(124.9331), [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])

5 tags: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])

After training: (tensor(83.8127), [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])

6 tags: tensor([0, 0, 0, 0, 2, 1, 1, 0, 0, 0, 0, 0])

After training: (tensor(62.5195), [0, 0, 0, 0, 2, 1, 0, 0, 0, 0, 0, 0])

7 tags: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])

After training: (tensor(144.1766), [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])

8 tags: tensor([0, 0, 0, 0, 0, 0, 0, 0])

After training: (tensor(42.8967), [0, 0, 0, 0, 0, 0, 0, 0])

9 tags: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0])

After training: (tensor(155.4373), [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])

10 tags: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])

After training: (tensor(76.5454), [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])

11 tags: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0])

After training: (tensor(136.9854), [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0])

12 tags: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])

After training: (tensor(68.2536), [0, 0, 0, 0, 0, 0, 0, 0, 0, 0])

13 tags: tensor([0, 0, 0, 0, 0])

After training: (tensor(24.8984), [0, 0, 0, 0, 0])

14 tags: tensor([0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0])

After training: (tensor(102.9605), [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])

15 tags: tensor([2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])

After training: (tensor(112.9822), [2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])

16 tags: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])

After training: (tensor(78.5971), [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])

17 tags: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])

After training: (tensor(67.9007), [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])

18 tags: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])

After training: (tensor(102.5593), [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])

19 tags: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])

After training: (tensor(110.1089), [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])

              precision    recall  f1-score   support

           0       0.98      1.00      0.99       296
           1       0.60      0.75      0.67         4
           2       1.00      0.44      0.62         9

   micro avg       0.98      0.98      0.98       309
   macro avg       0.86      0.73      0.76       309
weighted avg       0.98      0.98      0.97       309


Process finished with exit code 0



              precision    recall  f1-score   support

           0       0.95      0.99      0.97      4339
           1       0.68      0.32      0.43       132
           2       0.76      0.31      0.44       174

   micro avg       0.95      0.95      0.95      4645
   macro avg       0.80      0.54      0.62      4645
weighted avg       0.94      0.95      0.94      4645

